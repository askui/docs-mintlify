---
title: 'How AskUI Works'
description: 'Learn how AskUI works'
---

AskUI consists of mutliple components which all work together to allow users to build versatile automations across different operating systems. These components are:

* **Model Provider:** The Model Providers host AI models. The AI models are the brain of the Agents for Locating Elements (Grounding Models), Asking (Query Models) and autonoumous Acting (Large Action Models)

* **Agent Framework:** The Agent Framework is used to build an Agent in Python. You can easily interact with any OS by taking over the mouse and keyborad, visulizing the thinking process or use specialized tools like excel, pdf or web requests. 

* **Agent OS:** The Agent OS is installed on the device to automated, executing all commands and interacting with mouse and keyboard. Serves as the “hands” of the Vision Agent.

* **Agent Hub Web:** The Agent Hub Web provides you an easy way to coloborate with you're team. It allows you to sync your build agents over the repository to other devices. Let's trigger you agents from E-Mail or Web, schedule Agents on remote devices and manage your team.

* **Agent Hub Desktop:** The Agent Hub Desktop is GUI for your Agents. This is installed on your local device and allows you to setup your device, sync agents and interact with running agents. To be released Summer 2025.&#x20;

Lets have a deeper look at these components:

## Building Your Agent

The **Agent Framework** is where you **tell AskUI what to automate**. You can write single-step instructions or let the agent act on its own `agent.act("Book me flight from Karlsruhe to Rome")`.

For example, let’s say you want AskUI to click a "Login" button (single-step action). The instruction would look like this:

`my_agent.py`
```python
with VisionAgent(display=1) as agent: 
    agent.click("the Login button")
```
* **`VisionAgent(display=1)`** → Start and configure the agent (e.g. start a Vision Agent on display 1).
* **`agent`** → Specifies which agent is executing the instruction. 
* **`click("Element locator")`** → Specifies the action (clicking)


## Running your Agent

Here’s what’s happening under the hood why you now execute the agent with `python my_agent.py`:

### Start and configure the Vision Agent
```python
with VisionAgent(display=1) as agent:
```

### Run an Single Step Command
The with
```
agent.click("the Login button")
```
2. Connects to the Agent OS and take a screenshot.
2. Takes in a natural language locator of the to be located element. 
3. This locator is send with the screenshot to the configured AI model of the Model Providers. (Per default, we have set PTA-1 as a model hosted on AskUI Model Provider.)
4. The model is returning an AgentOS action. e.g. Click on pixel 100x 100y. 
5. This Action is the send to the AgentOS
6. The AgentOS is then take over the mouse and move the mouse to pixel 100x 100y and press the left mouse button

## Agent OS

Specifically, the Agent OS is doing the following when the agent is triggered:


The Agent OS furthermore has specific functions such as environment management, proxy configuration and in-background automation. A detailed overview can be found here. #todo

## Model Provider

AskUI supports multiple Model Providers and various different AI models for different purposes. Generally, these can be split into two:

* **UI Grounding Tasks:** This describes the process of using an AI model for localization of UI elements on a screen. Typically OCR, Computer Vision or Vision Language Models (VLMs) are used for this.
* **Reasoning and Planning:** To enable real agentic behaviour, a Large Language Model such as the Computer Use model by Anthropic is used for planning the steps to be executed with a reasoning loop.

We provide our own models as well as external Model Providers. You can find all supported models in #todo
