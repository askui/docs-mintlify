---
title: "Core Components"
description: "Understand the fundamental building blocks of AskUI"
---

The Vision Agent is the primary interface for interacting with user interfaces in AskUI. It represents the convergence of multiple AI technologies into a single, intelligent automation entity that can understand, reason about, and interact with visual interfaces.

## Vision Agent Overview

The Vision Agent combines several advanced technologies to create an intelligent automation system:

- **Computer Vision**: Analyzes screen content to identify and understand UI elements
- **Natural Language Processing**: Interprets human-like commands and instructions
- **AI Models**: Makes intelligent decisions about UI interactions and workflows
- **Context Awareness**: Maintains understanding of interface states and user goals

```python
from askui import VisionAgent

# Initialize the agent with configuration
with VisionAgent(
    log_level="INFO",  # Set logging level
    display=1,         # Use first display
    model="askui"      # Use default model
) as agent:
    # The agent is ready for both paradigms
    agent.click("login button")  # Single-step command
    agent.act("Complete the login process")  # Agentic instruction
```

## Core Capabilities

### Visual Understanding

The Vision Agent perceives interfaces through advanced computer vision:

- **Element Recognition**: Identifies buttons, fields, menus, and other UI components
- **Text Recognition**: Reads and understands text content in interfaces
- **Layout Analysis**: Understands spatial relationships between elements
- **State Detection**: Recognizes interface states, loading conditions, and changes

### Natural Language Processing

The agent understands human communication:

- **Command Interpretation**: Processes both specific commands and general instructions
- **Intent Recognition**: Understands what users want to accomplish
- **Context Integration**: Maintains conversation context across interactions
- **Ambiguity Resolution**: Handles unclear or incomplete instructions

### Intelligent Interaction

The agent makes smart decisions about how to interact:

- **Action Planning**: Determines the best sequence of actions to achieve goals
- **Error Recovery**: Adapts when actions don't produce expected results
- **Method Selection**: Chooses appropriate interaction methods for different situations
- **Timing Optimization**: Manages interaction timing for reliable automation

## Operating Modes

### Single-Step Command Mode

Execute precise, explicit instructions:

```python
# Direct, specific commands
agent.click("username field")
agent.type("username field", "john.doe")
agent.keyboard('tab')
agent.type("password field", "secret123")
agent.click("submit button")
```

### Agentic Instruction Mode

Provide high-level goals and let the agent figure out the details:

```python
# High-level, goal-oriented instructions
agent.act("Log in with username john.doe")
agent.act("Navigate to user settings")
agent.act("Enable two-factor authentication")
```

## Platform Specialization

Vision Agent serves as the base for platform-specific agents:

- **VisionAgent**: Optimized for desktop environments (Windows, macOS, Linux)
- **AndroidVisionAgent**: Specialized for Android mobile automation
- **Future Agents**: Platform-specific variants for emerging technologies

Learn more about [Agent Types](/03-explanation/01-foundations/agent-types) and their specializations.

## Information Extraction

The Vision Agent can extract information from interfaces:

```python
# Extract text content
error_message = agent.get("What is the error message?")
user_name = agent.get("What is the current user's name?")

# Get element locations
submit_location = agent.locate("submit button")
print(f"Submit button found at: {submit_location}")

# Extract structured data
table_data = agent.get("What are the values in the data table?")
```

## Built-in Tools Integration

The Vision Agent includes integrated tools for enhanced capabilities:

```python
# Web browser control
agent.tools.webbrowser.open_new("https://example.com")

# Operating system operations
agent.tools.os.copy_to_clipboard("automation data")
clipboard_content = agent.tools.os.paste_from_clipboard()

# Multi-monitor support
agent.tools.os.set_display(2)  # Switch to second display
```

## Configuration and Customization

### Display Configuration

```python
# Single display setup
with VisionAgent(display=1) as agent:
    agent.click("main window button")

# Multi-display setup
with VisionAgent(display=2) as agent:
    agent.click("secondary monitor element")
```

### Logging Configuration

```python
# Detailed logging for debugging
with VisionAgent(log_level="DEBUG") as agent:
    agent.act("Complete the workflow")

# Minimal logging for production
with VisionAgent(log_level="ERROR") as agent:
    agent.act("Execute batch process")
```

### Model Configuration

```python
# Default AskUI model
with VisionAgent(model="askui") as agent:
    agent.click("element")

# Custom model configuration
with VisionAgent(model="custom-model") as agent:
    agent.act("specialized task")
```

# Next Steps

- Understand **[Agent Types](/03-explanation/01-foundations/agent-types)** for platform-specific specializations
- Learn about **[Automation Paradigms](/03-explanation/01-foundations/automation-paradigms/overview)** to choose the right approach
- Explore **[Best Practices](/03-explanation/02-best-practices/01-element-selection/01-element-selection)** for effective automation
- Read about **[AI Models](/03-explanation/01-foundations/ai-models)** that power the Vision Agent
